{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213619bc-adf9-4777-bdb7-a2b0785f65bc",
   "metadata": {},
   "source": [
    "Web Scraping of University Reviews from Open Online Sources\n",
    "\n",
    "This project demonstrates how to scrape user-generated reviews from publicly available web platforms using Selenium and BeautifulSoup.\n",
    "\n",
    "**Key features:**\n",
    "- Automatically loads and reveals additional content via browser automation\n",
    "- Parses structured review data including rating, date, and full text\n",
    "- Saves extracted data to CSV and text files for further processing or analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561d068-07eb-4a13-a534-7910842e86d7",
   "metadata": {},
   "source": [
    " Required Libraries\n",
    "\n",
    "Below are the necessary Python libraries for web automation, HTML parsing, and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6a304-278e-42b1-b5cf-76f73ff7a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium and WebDriver components for browser automation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Common exceptions to handle during web scraping\n",
    "from selenium.common.exceptions import (\n",
    "    StaleElementReferenceException, NoSuchElementException,\n",
    "    ElementClickInterceptedException, TimeoutException\n",
    ")\n",
    "\n",
    "# Time module to add delays between browser actions\n",
    "import time\n",
    "\n",
    "# Pandas for working with tabular data (DataFrames)\n",
    "import pandas as pd\n",
    "\n",
    "# Regular expressions for text pattern matching and cleaning\n",
    "import re\n",
    "\n",
    "# BeautifulSoup for parsing and navigating HTML pages\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a33ddf-6f74-451f-ad58-d4ae2d664575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the university website and hide distracting advertisements\n",
    "def open_site_hide_ads(uni_url):\n",
    "    # Launch a new Chrome browser instance\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Navigate to the provided university URL\n",
    "    driver.get(uni_url)\n",
    "    \n",
    "    # Hide ads (iframes, Google ads, or dynamically loaded ad containers)\n",
    "    driver.execute_script(\"\"\"\n",
    "        document.querySelectorAll('iframe, ins.adsbygoogle, div[id^=\"aswift_\"]').forEach(el => el.style.display = 'none');\n",
    "    \"\"\")\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540453a9-7004-431c-9f14-4ff1ed2abbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function 3: Click the \"Show all reviews\" button using JavaScript ---\n",
    "def click_show_all_reviews(driver):\n",
    "    # Scroll and click the \"Show more\" button (if present) to load additional reviews\n",
    "    while True:\n",
    "        try:\n",
    "            # Hide ads again in case new ones loaded dynamically\n",
    "            driver.execute_script(\"\"\"\n",
    "                document.querySelectorAll('iframe, ins.adsbygoogle, div[id^=\"aswift_\"]').forEach(el => el.style.display = 'none');\n",
    "            \"\"\")\n",
    "            \n",
    "            # Wait until the \"Show more\" button is clickable\n",
    "            element = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.btn.blue\"))\n",
    "            )\n",
    "            \n",
    "            # Click the button and wait for the page to load new content\n",
    "            element.click()\n",
    "            time.sleep(2)\n",
    "\n",
    "        except (StaleElementReferenceException, NoSuchElementException, \n",
    "                ElementClickInterceptedException, TimeoutException):\n",
    "            print(\"No more 'Show more' button found or an issue occurred. Ending scroll.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c35c8-ff90-4d32-8fd6-95017fb1325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function 4: Expand all full review texts ---\n",
    "def read_all_reviews(driver):\n",
    "    # 1. Click all \"Read Review\" buttons to expand hidden review content\n",
    "    try:\n",
    "        buttons = driver.find_elements(By.XPATH, \"//a[contains(text(), 'Читати відгук')]\")\n",
    "        for btn in buttons:\n",
    "            try:\n",
    "                # Use JavaScript to click each button in case it's not directly clickable\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                time.sleep(0.5)  # Wait a bit for content to load\n",
    "            except Exception as e:\n",
    "                print(\"Failed to click button:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to find review buttons:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8b65f-4229-4ec0-b020-f5aea6d55bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reviews_full(driver):\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "    # Extract university name\n",
    "    uni_name = soup.find('h1', {'class': 'element_name'}).text\n",
    "\n",
    "    # Extract total number of reviews\n",
    "    reviews_count_tag = soup.find('div', {'class': 'rtngdescr'})\n",
    "    reviews_count = int(reviews_count_tag.find('span').text) if reviews_count_tag else \"No data\"\n",
    "\n",
    "    # Extract average rating\n",
    "    count_stars_tag = soup.find('span', {'class': 'average'})\n",
    "    count_stars = float(count_stars_tag.text) if count_stars_tag else \"No data\"\n",
    "\n",
    "    # Prepare to collect individual reviews\n",
    "    review_lst = []\n",
    "\n",
    "    # Locate all review blocks on the page\n",
    "    reviews = soup.find_all(\"div\", class_=\"comment_row\")\n",
    "    for review in reviews:\n",
    "        # Extract review date\n",
    "        date = review.find('span', {'class': 'value-title'})\n",
    "        date = date['title'] if date else \"No date\"\n",
    "\n",
    "        # Try to get full review text, fallback to snippet if not available\n",
    "        full_text_tag = review.find(\"span\", class_=\"review-full-text\")\n",
    "        if full_text_tag and full_text_tag.get_text(strip=True):\n",
    "            full_text = full_text_tag.get_text(strip=True)\n",
    "        else:\n",
    "            snippet_tag = review.find(\"span\", class_=\"review-snippet\")\n",
    "            full_text = snippet_tag.get_text(strip=True) if snippet_tag else \"No text\"\n",
    "\n",
    "        # Extract star rating from inline style (e.g., width: 39px → 3 stars)\n",
    "        stars_tag = review.find('span', {'class': 'star_ring'})\n",
    "        if stars_tag:\n",
    "            stars_width = stars_tag.find('span')['style']\n",
    "            stars_value = int(''.join(filter(str.isdigit, stars_width))) // 13\n",
    "        else:\n",
    "            stars_value = \"No rating\"\n",
    "\n",
    "        # Combine all extracted data into a dictionary\n",
    "        review_dict = {\n",
    "            'University Name': uni_name,\n",
    "            'Total Reviews': reviews_count,\n",
    "            'Average Rating': count_stars,\n",
    "            'Review Date': date,\n",
    "            'Review Rating': stars_value,\n",
    "            'Review Text': full_text\n",
    "        }\n",
    "        review_lst.append(review_dict)\n",
    "\n",
    "    # Save review texts to a .txt file for additional reference or inspection\n",
    "    safe_uni_name = re.sub(r'[\\\\/*?:\"<>|]', \"_\", uni_name)\n",
    "    filename = f\"{safe_uni_name}_reviews.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in review_lst:\n",
    "            f.write(r[\"Review Text\"] + \"\\n\\n\")\n",
    "    time.sleep(3) # Pause before exiting function\n",
    "    return review_lst, filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601ca65-7025-4173-b064-35d6dc2cd781",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns=['university_name', 'reviews_count', 'average_rating', 'review_date', 'review_rating', 'review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f88b31-c92f-4be2-9af2-75fb71becd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of university URLs to scrape reviews from\n",
    "links = [\n",
    "    # \"https://example-university1.edu/reviews\",\n",
    "    # \"https://example-university2.edu/reviews\",\n",
    "    # Add more university review page URLs here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0831b-83c4-43ff-8424-708375f723ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loop through all university URLs to collect reviews ---\n",
    "all_reviews = []             # List to store all parsed reviews\n",
    "saved_files_rewiev = []      # List to store file paths of saved reviews (typo corrected in comment)\n",
    "\n",
    "for uni_url in links:\n",
    "    # Open the university page and hide intrusive ads\n",
    "    driver = open_site_hide_ads(uni_url)\n",
    "\n",
    "    # Load all reviews by clicking \"Show More\"\n",
    "    click_show_all_reviews(driver)\n",
    "\n",
    "    # Expand full review content\n",
    "    read_all_reviews(driver)\n",
    "\n",
    "    # Parse all reviews from the current university page\n",
    "    reviews, file_path = parse_reviews_full(driver)\n",
    "\n",
    "    # Add parsed reviews to the full list\n",
    "    all_reviews.extend(reviews)\n",
    "\n",
    "    # Save file path to list (note: correct variable name below)\n",
    "    saved_files_rewiev.append(file_path)\n",
    "\n",
    "    # Close the browser for this iteration\n",
    "    driver.quit()\n",
    "\n",
    "# After gathering all reviews, convert to a pandas DataFrame\n",
    "final_df = pd.DataFrame(all_reviews)\n",
    "\n",
    "# Short pause to prevent script from finishing too fast\n",
    "time.sleep(3)\n",
    "\n",
    "# Preview the first 30 rows of collected reviews\n",
    "final_df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87854d-ddd6-49aa-842b-b0e8d58ef9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the collected reviews to a CSV file with UTF-8 encoding (with BOM for better compatibility with Excel)\n",
    "final_df.to_csv('reviews.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
